---
title: '论文解读Attention Is All You Need'
date: 2019-07-02
permalink: /posts/2019/07/blog-post-4/
tags:
  - NLP
  - Attention
---

Google提出的语言模型Transformer，抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。



## 动机

- 传统词向量训练所有词无论在哪个句子中的哪个位置，其词向量都是固定的，不能够表达句子以及其context的含义。

- RNN（或者LSTM，GRU等）的计算限制为是顺序的，也就是说RNN相关算法只能从向右依次计算或者从右向左依次计算，这种机制带来了两个问题：

  1. 时间片t的计算依赖t-1时刻的计算结果，这样限制了模型的并行能力；

  2. 顺序计算的过程中信息会丢失，尽管LSTM等门机制的结构一定程度上缓解了长期依赖的问题，但是对于特别长期的依赖现象，LSTM依旧无能为力。



## 框架

Transformer中抛弃了传统的CNN和RNN，整个网络结构完全是由Attention机制组成。更准确地讲，Transformer由且仅由self-Attenion和Feed Forward Neural Network组成。一个基于Transformer的可训练的神经网络可以通过堆叠Transformer的形式进行搭建，作者的实验是通过搭建编码器和解码器各6层，总共12层的Encoder-Decoder。



为了解决RNN的问题，transformer只用了attention机制，抛弃了传统RNN或CNN结构，将序列中的任意两个位置之间的距离是缩小为一个常量；其次它不是类似RNN的顺序结构，因此具有更好的并行性，符合现有的GPU框架。



## Attention Network

attention起源于cv，在nlp领域最早用于seq2seq与机器翻译。在使用RNN作为encoder时每个token输出一个定长的hidden representation，如果要在decoder端根据整个句子的信息进行预测每个翻译出来的词的概率，仅通过encoder端最终输出的一个定长向量很难表达token之间的交互信息。attention解决了这个信息瓶颈问题 [1]：

> An attention network maintains a set of hidden representations that scale with the size of the source. The model uses an internal inference step to perform a soft-selection over these representations. This method allows the model to maintain a variable-length memory and has shown to be crucially important for scaling systems for many tasks.

给定input序列 $x=[x_1, x_2, ..., x_n]$，q是一个query，z是一个categorical variable，其取值是{1,...,n}，代表根据q从x中选择相关的input。attention的目标就是根据q和x的信息生成一个上下文c。通过计算z的分布$z\sim p(z\rvert x, q)$，c可以写为期望的形式：

$$c=\mathbb{E}_{z\sim p(z\rvert x,q)}[f(x,z)]$$

$f(x,z)$叫做annotation function，定义了x和z如何组合。具体来说，在一个基于RNN的机器翻译网络中，input x就是encoder RNN中每个节点的隐层，q就是decoder RNN中某个节点的隐层，z代表了翻译原文input的位置，也就是与当前decoder输入的词最相关的input的词。此时c的计算可写为：

$$c=\sum_{i=1}^n$p(z=i\rvert x,q)x_i$$

其中$p(z=i\rvert x,q)=softmax(MLP(x_i; q))$。即根据q和x的关系来线性组合x。如果是self-attention的话，q就是x本身。

![](https://camo.githubusercontent.com/a3c087bfca2eb44553f284ac13b19ccc27a333ed/68747470733a2f2f332e62702e626c6f6773706f742e636f6d2f2d3350626a5f64767430566f2f562d71652d4e6c365035492f41414141414141414251632f7a305f365774565774764152744d6b3069395f41744c6579794779563641493477434c63422f73313630302f6e6d742d6d6f64656c2d666173742e676966)







