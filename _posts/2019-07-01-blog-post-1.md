---
title: 'Deep Reinforcement Learning: Pong from Pixels学习笔记'
date: 2019-07-1
permalink: /posts/2019/07/blog-post-1/
tags:
  - 强化学习
  - 学习笔记
---

Deep Reinforcement Learning: Pong from Pixels是一篇很好的强化学习入门博文。其中详细介绍了DRL一种常用的方法：Policy Gradient。原文链接：http://karpathy.github.io/2016/05/31/rl/

# 前言

# 乒乓球游戏

一句话描述Policy gradient: 使用一种policy跑一会儿，然后看看这种policy下的哪种行为会得到高分，然偶增加这种行为的概率。

## 与有监督学习的关系

其实PG与有监督学习非常类似，只有2个细微的差别。

1. 有监督学习在得到logit并softmax之后，可以通过和真实标签计算交叉熵得到loss再计算梯度以及使用反向传播更新神经网络。然而在PG的学习过程中，我们并没有真实标签y，softmax之后得到的是产生每个action的概率，例如向上移动0.7，向下移动0.3。由于没有标签，只能通过对action的概率进行采样，采样到的action就是它的标签，所以PG的标签可以称为“假标签”。
2. 采样得到的action应用到环境中，然后通过环境给出的reward来计算梯度。所以PG的loss计算其实就是$A_i\log p(y_i|x_i)$，$A_i$是在当前环境$x_i$下通过policy $p(x)$采样得到的行为$y_i$。因为一个step只能有一个action，所以softmax的其他项都是0（我们不可能在当前step计算每个action的$A_i$）。

## 折扣Reward

一个回合中，每个step都希望考虑除了本身的reward，还加上一些未来的reward，但是离当前step越远的reward的权重就越低。这样做使得早期的action与最终一个episode的结局reward有了联系，尤其是在一些场景中，每个step没有即时的reward，只有当一个episode结束后才能知道这一系列action将得到多少reward。
$$
\begin{align}
R_t=&\sum_{k=0}^\inf \gamma^kr_{t+k}\\
=&r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + ...
\end{align}
$$

## PG的数学推导

PG的loss，或者说是最终得分其实很简单，就是$E_{x\sim p(x|\theta)}[f(x)]$，就是在当前policy $p(x)$下的得分$f(x)$的期望。所以我们需要计算的就是得分期望的梯度$\nabla_\theta E_{x\sim p(x|\theta)}[f(x)]$。推导如下：

![image-20190701112717103](./Deep Reinforcement Learning Pong from Pixels学习笔记.assets/image-20190701112717103.png)

其实本质上就是如何将求梯度从期望外移进去，并且再写成期望的形式的一个过程。从最后公式推导最后一行可以看出，最终的形式是每一步的概率求log之后计算梯度，再乘上对应的得分，最后再求均值。